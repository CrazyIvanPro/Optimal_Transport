% Section X: Optimal Transport and Wasserstein GAN: A Machine Learning Perspective

\vspace{10ex}
\section{Optimal Transport and Wasserstein GAN: A Machine Learning Perspective}
Optimal Transport is widely used in applied ML, such as a large class of generative models. Among its many successful cases, Wasserstein GAN is supported by OT. Starting from theoretical analysis,  with small changes we can solve the training stability problem of original GAN , collapse mode problem, and so on. 

\subsection{Optimal Transport Divergence}
In many cases, what we need and care about is not the mapping $\mathbf{\pi}$ here, but the final minimum cost (the minimum cost of transportation). For example, when we need to measure the distance between two probability distributions, we can use this optimal transport divergence as an effective metric. In original Wasserstein GAN\cite{Arjovsky2017}, the author found:

Under the optimal discriminator of the original GAN, minimizing the loss of the generator is equivalent to minimizing the JS divergence of the distribution generated by the generator and the target distribution. But there is a fatal flaw in using JS divergence as a metric, that is, in the case where the two distributions do not intersect each other, the JS divergence of the two distributions is always a constant $\log 2$,  and due to the generator generated distribution and the target support set It is a low-dimensional manifold embedded in a high-dimensional space, so the measure of their overlapping part is almost 0. This makes it impossible to measure the distance between two distributions disjointed. There is also a zero gradient when computing gradients. Because JS divergence and KL divergence both suffer from the above metric problems, this method of measuring distance is not continuous and derivable everywhere. But our GANs need metric methods that are differentiable and continuous. 

Using optimal transport divergence as a measurement method can solve the above problems of JS divergence and KL divergence. (The distance between the two distributions $P$, $Q$ can be measured from another angle, that is, the distance between the two distributions is defined as the minimum cost to transport from the distribution P to the distribution $Q$.)

Optimal Transport Divergence is defined as follows:
\begin{equation}
  \label{eq:OTD}
  O T(P \| Q)=\inf _{\pi} \int_{X \times Y} \pi(x, y) c(x, y) d x d y  
\end{equation}

Constraints are $\int_{y} \pi(x, y) d y=P(x)$ and $\int_{x} \pi(x, y) d x=Q(y)$.

In order to calculate the square of the commonly used $L_2$ norm to define the cost, that is, $c(x,y) = ||x - y||_2^2$, then we can get the 2-Wasserstein Distance
\begin{equation}
  W_{2}(P, Q)=\inf _{\pi} \int_{X \times Y} \pi(x, y)\|x-y\|_{2}^{2} d x d y  
\end{equation}

In a more general case, the k-Wasserstein Distance is:
\begin{equation}
  W_{k}(P, Q)=\inf _{\pi} \int_{X \times Y} \pi(x, y)\|x-y\|_{k}^{k} d x d y  
\end{equation}

Although we got this kind of good-quality measurement method, if we look further, we will find some problems. Then as the dimension of the input random variable that needs to be compared grows, the problem size grows exponentially. For example, for a binary image of size $32 \times 32$, then the dimension of each input random variable has nearly 1000, then the scale of the problem we have now is $O(2^{1024})$, which is impossible to calculate.

\subsection{Dual Form}
The Optimal transport problem is actually a linear programming problem because the problems we need to optimize and their constraints are linear functions. And for any linear programming problem, it can be transformed into its dual problem, and the solution of the dual problem is the lower bound of the optimal solution of the original problem.

Then the dual problem of Optimal transport problem is defined as follows:
\begin{equation}
  OT(P \| Q)=\sup _{f \in L}\left(\int \psi(x) P(x) d x+\int \phi(y) Q(y) d y\right)  
\end{equation}

where $L=\{f: \mathfrak{R} \rightarrow \mathfrak{R} | \psi(x)+\phi(y) \leq c(x, y)\}$.

Because the original problem \ref{eq:OTD} contains constraints, we need to find a way to remove it first, so consider the following questions:
\begin{equation}
    \label{eq:p2}
  \sup _{\psi}\left(\int_{x^{\prime}} \psi\left(x^{\prime}\right) P\left(x^{\prime}\right) d x^{\prime}-\int_{x} \int_{y} \psi(x) \pi(x, y) d x d y\right)
\end{equation}

The problem $\int_{x^{\prime}} \psi\left(x^{\prime}\right) P\left(x^{\prime}\right) d x^{\prime}$ is the expectation of $\psi$ under the distribution $P$, and $\int_{x} \int_{y} \psi(x) \pi(x, y) d x d y$ is the expectation of $\psi$ under the marginal probability distribution $\int_{y} \pi(x, y) d y$. Obviously, when $\int_{y} \pi(x, y) d y=P(x)$, for any function $\psi$, formula \ref{eq:p2} is 0; if it is not equal, it is easy to find $\psi$ to make formula \ref{eq:p2} also infinite. The same principle applies

\begin{equation}
    \label{eq:p3}
  \sup _{\phi}\left(\int_{y^{\prime}} \phi\left(y^{\prime}\right) Q\left(y^{\prime}\right) d y^{\prime}-\int_{x} \int_{y} \phi(y) \pi(x, y) d x d y\right)
\end{equation}

It can be found that if you add \ref{eq:p2} \ref{eq:p3} to the original problem \ref{eq:OTD}, in the case of $\int_{y} \pi(x, y) d y=P(x)$ and $\int_{x} \pi(x, y) d x=Q(y)$,  it will not affect the optimization problem. If it is not satisfied, there is no solution. At the same time, with a little transformation on \ref{eq:p2} \ref{eq:p3}, we can get

\begin{equation}
\begin{array}{l}{\sup _{\psi}\left(\int_{x} \int_{y}\left[\int_{x^{\prime}} \psi\left(x^{\prime}\right) P\left(x^{\prime}\right) d x^{\prime}-\psi(x)\right] \pi(x, y) d x d y\right)} \\ {\sup _{\phi}\left(\int_{x} \int_{y}\left[\int_{y^{\prime}} \phi\left(y^{\prime}\right) Q\left(y^{\prime}\right) d y^{\prime}-\phi(y)\right] \pi(x, y) d x d y\right)}\end{array}
\end{equation}

Because $\int_{x^{\prime}} \psi\left(x^{\prime}\right) P\left(x^{\prime}\right) d x^{\prime}$ is constant for $\int_{x} \int_{y} \psi(y) \pi(x, y) d x d y$, the expectation of the constant is the constant itself.

Let
\begin{equation}
  L=\int_{x} \int_{y}\left[\int_{x^{\prime}} \psi\left(x^{\prime}\right) P\left(x^{\prime}\right) d x^{\prime}-\psi(x)\right] \pi(x, y) d x d y+\int_{x} \int_{y}\left[\int_{y^{\prime}} \phi\left(y^{\prime}\right) Q\left(y^{\prime}\right) d y^{\prime}-\phi(y)\right] \pi(x, y) d x d y
\end{equation}

So after adding it to \ref{eq:OTD}, we get:
\begin{equation}
  \inf _{\pi}\left[\int \pi(x, y) c(x, y) d x d y+\sup _{\psi, \phi} L\right]
\end{equation}

Because formula above is a linear function of $\psi, \phi$ and $\pi$, Sion's minimax theorem guaranteed that the order could be exchanged:

\begin{equation}
    \label{eq:sion}
  \sup _{\psi, \phi}\left[\int_{x^{\prime}} \psi\left(x^{\prime}\right) P\left(x^{\prime}\right) d x^{\prime}+\int_{y^{\prime}} \phi\left(y^{\prime}\right) Q\left(y^{\prime}\right) d y^{\prime}+\inf _{\pi} \int_{y}[c(x, y)-(\psi(x)+\phi(y))] \pi(x, y) d x d y\right]
\end{equation}

Let $l=c(x, y)-(\psi(x)+\phi(y))$, then formula \ref{eq:sion} for the optimization problem of $\pi$, if $l \geq 0$, then for any $x, y$, 

\begin{equation}
  \inf _{\pi} \int_{y}[c(x, y)-(\psi(x)+\phi(y))] \pi(x, y) d x d y=0
\end{equation}

In order to have a solution, Equation \ref{eq:sion} can be transformed into a problem with constraints, as follows:

\begin{equation}
\begin{array}{l}{\sup _{\psi, \phi \in L}\left(\int \psi(x) P(x) d x+\int \phi(y) Q(y) d y\right) \text { s.t. }} \\ {L=\{\psi, \phi: \mathfrak{R} \rightarrow \mathfrak{R} | \psi(x)+\phi(y) \leq c(x, y)\}}\end{array}
\end{equation}

\subsection{Solving the OT Divergence Problem}
Next, we will introduce an effective method for solving OTD, and add regularization to the original problem \ref{eq:OTD}. The idea of adding regularization to the original problem can be traced back to Wilson's research on traffic patterns in 1969. They found that the traffic pattern of a transportation network actually does not meet the optimal solution they obtained through the optimal transport problem. They found that the true traffic pattern is more diffuse than the prediction of the optimal transport problem, which means that the true traffic pattern depends on many paths instead of the optimal results obtained by the optimal transport problem, which depends on only a few paths.

After adding entropic regularization, we will get several benefits: firstly, dual problems with entropic regularization are smooth. (can be used as loss function directly). Secondly, the problem solved becomes an unconstrained, convex problem.

Looking back, we let regularized optimal transport as follows:

\begin{equation}
  O T_{c, \lambda}(P, Q)=\min _{\pi \in U(P, Q)} \int_{a} \int \pi(x, y) c(x, y) d x d y+\varepsilon E(\pi)
\end{equation}

Here $E(\pi)$ is a regularization term, and commonly used are relative entropy, $L_2$ norm, etc. Taking relative entropy as an example, $E(\pi)=\int_{x} \int_{y} \pi(x, y) \log \left(\frac{\pi(x, y)}{P(x) Q(y)}\right) d x d y$, then formula (13) is:

\begin{equation}
\begin{aligned} O T_{c, \lambda}(P, Q)=& \min _{\pi \in U(P, Q)} \int_{x} \int_{y} \pi(x, y) c(x, y) d x d y+\varepsilon \int_{x} \int_{y} \pi(x, y) \log \left(\frac{\pi(x, y)}{P(x) Q(y)}\right) d x d y \\ & \text { s.t. } \int_{y} \pi(x, y) d y=P(x), \int_{x} \pi(x, y) d x=Q(y) \end{aligned}
\end{equation}

  Let $\psi(x)$ and $\phi(y)$ be Lagrange multipliers with two constraints, respectively. Then we have

\begin{equation}
\begin{array}{l}{\min _{\pi \geq 0} \int_{x} \int_{y}\left[\pi(x, y) c(x, y)+\varepsilon \pi(x, y) \log \left(\frac{\pi(x, y)}{P(x) Q(y)}\right)\right.} \\ {+\psi(x)(\pi(x, y)-P(x))+\phi(y)(\pi(x, y)-Q(y))] d x d y}\end{array}
\end{equation}

  And because of $\pi(x,y) \geq 0$, then there is,

  \begin{equation}
    \pi^{*}(x, y)=P(x) Q(y) \exp \left(-\frac{1}{\varepsilon}(c(x, y)+\psi(x)+\phi(y))+1\right)
  \end{equation}

  Similarly we can get its dual problem

  \begin{equation}
  \max _{\psi, \phi} \int_{x} \psi(x) P(x) d x+\int_{y} \phi(y) Q(y) d y+\frac{\varepsilon}{e} \int_{x} \int_{y} \exp \left(-\frac{1}{\varepsilon}(c(x, y)+\psi(x)+\phi(y))\right) \pi(x, y) d x d y
\end{equation}